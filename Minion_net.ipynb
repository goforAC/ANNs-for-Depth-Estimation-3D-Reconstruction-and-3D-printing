{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label / target preprocessing / splitting\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "\n",
    "n_points = 2000\n",
    "shuffle_data = True\n",
    "\n",
    "def target_four_hot_decoder(n_points):\n",
    "    # four_hot encoder\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(1,n_points+1):\n",
    "        filename = './simple_minion_dataset/targets/target%i.txt' % (i)\n",
    "        temp = np.array([0,0,  0,0,0,0,0,0,0,0,0,0,  0,0,0,0,0,0,0,0,0,  0,0,0,0,0,0,0,0,0,0])\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                items = line.split(':')\n",
    "                key, value = items[0], float(items[1])\n",
    "                if key == \"Eyes\":\n",
    "                    temp[int(value)] = 1\n",
    "                elif key == \"Height\":\n",
    "                    temp[int(value*10+1)] = 1\n",
    "                elif key == \"Arm_Length\":\n",
    "                    temp[int(value*10+10)] = 1\n",
    "                elif key == \"Leg_Length\":\n",
    "                    temp[int(value*10+20)] = 1\n",
    "        labels.append(temp)\n",
    "\n",
    "    return labels\n",
    "\n",
    "def taget_four_units_decoder(n_points):\n",
    "    #decodes targets as the floating point values they are\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(1,n_points+1):\n",
    "        filename = './simple_minion_dataset/targets/target%i.txt' % (i)\n",
    "        temp = np.array([])\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                items = line.split(':')\n",
    "                key, value = items[0], float(items[1])\n",
    "                temp = np.append(temp, value)\n",
    "        labels.append(temp)\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "minion_files = [(\"./simple_minion_dataset/data/minion%d.png\" % i) for i in range(1,n_points+1)]\n",
    "minion_targets = taget_four_units_decoder(n_points)\n",
    "        \n",
    "# shuffle data if desired\n",
    "if shuffle_data:\n",
    "    packed = list(zip(minion_files, minion_targets))\n",
    "    shuffle(packed)\n",
    "    minion_files, minion_targets = zip(*packed)\n",
    "    \n",
    "# devide data into 60:20:20\n",
    "train_minion_files = minion_files[0:int(0.6*len(minion_files))]\n",
    "train_minion_targets = minion_targets[0:int(0.6*len(minion_targets))]\n",
    "\n",
    "val_minion_files = minion_files[int(0.6*len(minion_files)):int(0.8*len(minion_files))]\n",
    "val_minion_targets = minion_targets[int(0.6*len(minion_targets)):int(0.8*len(minion_targets))]\n",
    "\n",
    "test_minion_files = minion_files[int(0.8*len(minion_files)):]\n",
    "test_minion_targets = minion_targets[int(0.8*len(minion_targets)):]\n",
    "    \n",
    "\n",
    "                \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.data import Dataset, Iterator\n",
    "from itertools import count\n",
    "from datetime import datetime\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "BatchSize = 10  # tf.constant(10, dtype=tf.int64)\n",
    "Epochs = 100  # tf.constant(50, dtype=tf.int64)\n",
    "Height = 100\n",
    "Width = 100\n",
    "Channels = 3\n",
    "Validation_Size = len(val_minion_files)\n",
    "\n",
    "eyes_accuracy = []\n",
    "height_accuracy = []\n",
    "arms_accuracy = []\n",
    "legs_accuracy = []\n",
    "\n",
    "def read_from_file(filename, target):\n",
    "    file_content = tf.read_file(filename)\n",
    "    decoded = tf.image.decode_png(file_content, channels=3, dtype=tf.uint8)\n",
    "    normalised = tf.image.convert_image_dtype(decoded, tf.float32)\n",
    "    resized = tf.image.resize_images(normalised, (Height, Width))\n",
    "    \n",
    "    return resized, target\n",
    "\n",
    "\n",
    "# Get training dataset into TensorFlow\n",
    "minion_train = Dataset.from_tensor_slices((\n",
    "    tf.convert_to_tensor(train_minion_files), \n",
    "    tf.convert_to_tensor(np.array(train_minion_targets).astype(np.float32))\n",
    "    ))\n",
    "\n",
    "minion_train = minion_train.map(read_from_file, num_threads=2, output_buffer_size=200)\n",
    "minion_train = minion_train.shuffle(buffer_size=20000) # another way to shuffle data\n",
    "minion_train = minion_train.batch(BatchSize)\n",
    "minion_train = minion_train.repeat(Epochs)\n",
    "\n",
    "# Get test dataset into TesorFlow\n",
    "minion_test = Dataset.from_tensor_slices((\n",
    "    tf.convert_to_tensor(test_minion_files),\n",
    "    tf.convert_to_tensor(np.array(test_minion_targets).astype(np.float32))\n",
    "    ))\n",
    "\n",
    "minion_test = minion_test.map(read_from_file, num_threads=2, output_buffer_size=200)\n",
    "minion_test = minion_test.shuffle(buffer_size=20000)\n",
    "minion_test = minion_test.batch(BatchSize)\n",
    "minion_test = minion_test.repeat(Epochs)\n",
    "\n",
    "# Get validation dataset into TesorFlow\n",
    "minion_val = Dataset.from_tensor_slices((\n",
    "    tf.convert_to_tensor(val_minion_files),\n",
    "    tf.convert_to_tensor(np.array(val_minion_targets).astype(np.float32))\n",
    "    ))\n",
    "\n",
    "minion_val = minion_val.map(read_from_file, num_threads=2, output_buffer_size=200)\n",
    "# minion_val = minion_val.shuffle(buffer_size=20000) # no shuffle neccessary\n",
    "minion_val = minion_val.batch(Validation_Size)\n",
    "minion_val = minion_val.repeat(1) # no repeat necessary\n",
    "\n",
    "train_iterator_handle = minion_train.make_one_shot_iterator().string_handle()\n",
    "test_iterator_handle  = minion_test.make_one_shot_iterator().string_handle()\n",
    "val_iterator_handle   = minion_val.make_one_shot_iterator().string_handle()\n",
    "\n",
    "# Train and Test interators and handles\n",
    "dataset_handle          = tf.placeholder(tf.string, shape=[])\n",
    "iterator                = Iterator.from_string_handle(dataset_handle, (tf.float32,tf.float32), ((None, 100, 100, 3), (None, 4)))\n",
    "imagebatch, targetbatch = iterator.get_next()\n",
    "\n",
    "## Net Construction\n",
    "\n",
    "# first conv_relu layer\n",
    "h1 = tf.layers.conv2d(imagebatch, filters=32, kernel_size=5, strides=(4,4), \n",
    "                    activation=tf.nn.relu)\n",
    "# second conv_relu layer\n",
    "h2 = tf.layers.conv2d(h1, filters=64, kernel_size=2, strides=(2,2), \n",
    "                    activation=tf.nn.relu)\n",
    "# flatten for dense\n",
    "h2_flat = tf.contrib.layers.flatten(h2)\n",
    "\n",
    "# first dense layer\n",
    "h3 = tf.layers.dense(h2_flat, units=512, activation=tf.nn.tanh)\n",
    "\n",
    "# add dropout\n",
    "keep_prob = tf.placeholder(tf.float32, shape=[])\n",
    "h3_drop = tf.nn.dropout(h3, keep_prob)\n",
    "\n",
    "# dense output layer\n",
    "net_out = tf.layers.dense(h3_drop, units=4, activation=tf.nn.sigmoid)\n",
    "\n",
    "loss = tf.losses.mean_squared_error(targetbatch, net_out)\n",
    "tf.summary.scalar('MSE_loss', loss)\n",
    "train_step = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training Session\n",
    "with tf.Session() as sess:\n",
    "    time = datetime.now().strftime('%y%m%d-%H%M%S')    \n",
    "    train_writer      = tf.summary.FileWriter('Summaries/%s/train' % time, sess.graph)\n",
    "    test_writer       = tf.summary.FileWriter('Summaries/%s/test' % time)\n",
    "    validation_writer = tf.summary.FileWriter('Summaries/%s/validation' % time)\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    trainhandle_string = sess.run(train_iterator_handle) # evaluate to accepted handla format\n",
    "    testhandle_string  = sess.run(test_iterator_handle)\n",
    "    valhandle_string   = sess.run(val_iterator_handle)\n",
    "    for i in count():\n",
    "        try:\n",
    "            _ = sess.run( train_step, {dataset_handle: trainhandle_string, keep_prob: 0.5})\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "        \n",
    "        if not i % 200:\n",
    "            train_summary = sess.run(merged, {dataset_handle: trainhandle_string, keep_prob: 1.0})\n",
    "            train_writer.add_summary(train_summary, i)\n",
    "            test_summary, net_out_batch, target_batch = sess.run([merged, net_out, targetbatch], {dataset_handle: testhandle_string, keep_prob: 1.0})\n",
    "            test_writer.add_summary(test_summary, i)\n",
    "            \n",
    "            \n",
    "            \n",
    "            eyes   = np.equal(np.round(net_out_batch[:,0]), target_batch[:,0])\n",
    "            eyes_accuracy.append(np.mean(eyes))\n",
    "            height = np.equal(np.round(net_out_batch[:,1],1), target_batch[:,1])\n",
    "            height_accuracy.append(np.mean(height))\n",
    "            arms   = np.equal(np.round(net_out_batch[:,2],1), target_batch[:,2])\n",
    "            arms_accuracy.append(np.mean(arms))\n",
    "            legs   = np.equal(np.round(net_out_batch[:,3],1), target_batch[:,3])\n",
    "            legs_accuracy.append(np.mean(legs))\n",
    "            \n",
    "    validation_summary, val_net_out, val_target_batch = sess.run([merged, net_out, targetbatch], {dataset_handle: valhandle_string, keep_prob: 1.0})\n",
    "    validation_writer.add_summary(validation_summary, i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featurewise performance during training of the ANN on test data batches: \n",
      "[[ 0.6  0.   0.   0.2]\n",
      " [ 1.   1.   0.9  0.7]\n",
      " [ 1.   1.   1.   0.6]\n",
      " [ 1.   1.   1.   0.5]\n",
      " [ 1.   1.   0.9  0.6]\n",
      " [ 1.   1.   0.9  1. ]\n",
      " [ 1.   1.   1.   1. ]\n",
      " [ 1.   1.   1.   0.8]\n",
      " [ 1.   1.   1.   0.8]\n",
      " [ 1.   1.   1.   0.7]\n",
      " [ 1.   1.   1.   0.6]\n",
      " [ 1.   1.   1.   0.9]\n",
      " [ 1.   1.   1.   0.9]\n",
      " [ 1.   1.   1.   0.9]\n",
      " [ 1.   1.   1.   0.7]\n",
      " [ 1.   1.   1.   1. ]\n",
      " [ 1.   1.   1.   0.9]\n",
      " [ 1.   1.   1.   1. ]\n",
      " [ 1.   1.   1.   0.7]\n",
      " [ 1.   1.   1.   0.7]\n",
      " [ 1.   1.   1.   0.8]\n",
      " [ 1.   1.   1.   1. ]\n",
      " [ 1.   1.   1.   1. ]\n",
      " [ 1.   1.   1.   0.9]\n",
      " [ 1.   1.   1.   1. ]\n",
      " [ 1.   1.   1.   0.9]\n",
      " [ 1.   1.   1.   1. ]\n",
      " [ 1.   1.   1.   1. ]\n",
      " [ 1.   1.   1.   0.8]\n",
      " [ 1.   0.9  0.9  1. ]\n",
      " [ 1.   1.   1.   1. ]\n",
      " [ 1.   1.   1.   0.9]\n",
      " [ 1.   1.   1.   1. ]\n",
      " [ 1.   1.   1.   1. ]\n",
      " [ 1.   1.   1.   0.8]\n",
      " [ 1.   1.   1.   0.9]\n",
      " [ 1.   1.   1.   1. ]\n",
      " [ 1.   1.   1.   1. ]\n",
      " [ 1.   1.   1.   1. ]\n",
      " [ 1.   1.   1.   0.9]\n",
      " [ 1.   1.   1.   1. ]\n",
      " [ 1.   1.   1.   1. ]\n",
      " [ 1.   1.   1.   0.8]\n",
      " [ 1.   1.   1.   1. ]\n",
      " [ 1.   1.   1.   1. ]\n",
      " [ 1.   1.   1.   1. ]\n",
      " [ 1.   1.   1.   1. ]\n",
      " [ 1.   1.   1.   1. ]\n",
      " [ 1.   1.   1.   1. ]\n",
      " [ 1.   1.   1.   1. ]\n",
      " [ 1.   1.   1.   0.9]\n",
      " [ 1.   1.   1.   1. ]\n",
      " [ 1.   1.   1.   0.9]\n",
      " [ 1.   0.9  0.9  1. ]\n",
      " [ 1.   1.   1.   1. ]\n",
      " [ 1.   1.   0.8  0.6]\n",
      " [ 1.   1.   1.   0.9]\n",
      " [ 1.   1.   1.   0.9]\n",
      " [ 1.   1.   1.   1. ]\n",
      " [ 1.   1.   1.   0.9]]\n",
      "Number of tests: 60\n",
      "Performance on validation data: 95 %\n",
      "Featurewise performance on validation data: \n",
      "eyes, height, arm length, leg length\n",
      "[ 100.   100.   100.    95.5]\n"
     ]
    }
   ],
   "source": [
    "# batchwise performance on testing data during training on eyes, height, arm_length, leg_length\n",
    "batch_performance = np.transpose([eyes_accuracy,\n",
    "                                     height_accuracy,\n",
    "                                     arms_accuracy,\n",
    "                                     legs_accuracy])\n",
    "\n",
    "print('Featurewise performance during training of the ANN on test data batches: ')\n",
    "print(batch_performance)\n",
    "print('Number of tests: %d' % batch_performance.shape[0])\n",
    "\n",
    "val_net_out_round = np.concatenate((np.expand_dims(np.round(val_net_out[:,0]).T,axis=1),np.round(val_net_out[:,1:],1)), axis=1)\n",
    "val_results = np.equal(val_net_out_round, val_target_batch)\n",
    "\n",
    "val_results_total = np.all(val_results, axis=1)\n",
    "validation_performance = np.mean(val_results_total)\n",
    "\n",
    "validation_performance_featurewise = np.mean(val_results, axis=0)\n",
    "\n",
    "print('Performance on validation data: %d %%' % (validation_performance*100))\n",
    "\n",
    "print('Featurewise performance on validation data: ')\n",
    "print('eyes, height, arm length, leg length')\n",
    "print(validation_performance_featurewise*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
